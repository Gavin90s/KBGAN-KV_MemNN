{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some parameters of this model\n",
    "N, D_in, H = 64, 20, 10\n",
    "safety_margin_size = 3\n",
    "Max_sampleNum = 50\n",
    "total_item_num = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return (a-b).pow(2).sum(1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_memory = torch.randn(H, D_in, requires_grad=True)\n",
    "key_memory = torch.randn(H, D_in, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(500, D_in)\n",
    "y = torch.randn(500, D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.len = x.shape[0]\n",
    "        self.x_data = torch.as_tensor(x,dtype=torch.float)\n",
    "        self.y_data = torch.as_tensor(y,dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = CandidateDataset(x, y)\n",
    "train_loader = DataLoader(dataset = my_dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    def __init__(self, D_in):\n",
    "        super(generator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2*D_in, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)  # Prob of Left\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        cat_xy = torch.cat([x,y],1)\n",
    "        hidden1 = F.relu(self.fc1(cat_xy))\n",
    "        hidden2 = F.relu(self.fc2(hidden1))\n",
    "        output = torch.sigmoid(self.fc3(hidden2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, N, key_memory, value_memory):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.key_memory = Parameter(torch.Tensor(H, D_in))\n",
    "        self.value_memory = Parameter(torch.Tensor(H, D_in))\n",
    "        self.key_memory.data = key_memory\n",
    "        self.value_memory.data = value_memory\n",
    "        #self.value_memory.requires_grad=True\n",
    "        self.fc_erase = nn.Linear(D_in, D_in)\n",
    "        self.fc_update = nn.Linear(D_in, D_in)\n",
    "        self.D_in = D_in\n",
    "        self.N = N\n",
    "        self.H = H\n",
    "    def forward(self, user, item):\n",
    "        output_list = []\n",
    "        e_I = torch.ones(self.H, self.D_in)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            #Memory Adderssing\n",
    "            Attention_weight = torch.empty(self.H)\n",
    "\n",
    "            for j in range(self.H):\n",
    "                Attention_weight[j] = -euclidean_distance(user[i].unsqueeze(0), self.key_memory[j,:])\n",
    "\n",
    "\n",
    "            #select value memory by attention\n",
    "\n",
    "            Attention_weight = Attention_weight.softmax(0)\n",
    "            s = Attention_weight.matmul(self.value_memory)\n",
    "\n",
    "            output = euclidean_distance(s, item[i].unsqueeze(0))\n",
    "            output_list.append(output)\n",
    "            \n",
    "            #update value memory by item vector\n",
    "            e_t = self.fc_erase(item[i].unsqueeze(0)).sigmoid()\n",
    "            self.value_memory.data = self.value_memory * (e_I - Attention_weight.unsqueeze(0).t().matmul(e_t))\n",
    "\n",
    "            a_t = self.fc_update(item[i].unsqueeze(0)).tanh()\n",
    "            self.value_memory.data = self.value_memory + Attention_weight.unsqueeze(0).t().matmul(a_t)\n",
    "        \n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngtv_spl_by_unifm_dist(user):\n",
    "    ngtv_item = torch.randn(1, D_in)\n",
    "    return ngtv_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngtv_spl_by_generator(user):\n",
    "    ngtv_item_id = list(torch.multinomial(prob_ngtv_item, 1, replacement=True))[0]\n",
    "    ngtv_item = dict_item_id2vec[candidate_ngtv_item[ngtv_item_id].item()]\n",
    "    return ngtv_item_id, ngtv_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_pretrain_per_datapoint(user, generator, model):\n",
    "    ngtv_item_id, ngtv_item = generator(user)\n",
    "    neg_rslt = model(user.unsqueeze(0), ngtv_item)[0]\n",
    "    return -neg_rslt, ngtv_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_pretrain_per_datapoint(user, item, generator):\n",
    "    ps_rslt = model(user.unsqueeze(0),item.unsqueeze(0))[0]\n",
    "    for i in range(Max_sampleNum):\n",
    "        weight = log((total_item_num-1)//(i+1) + 1)\n",
    "        neg_rslt = model(user.unsqueeze(0), generator(user))[0]\n",
    "        loss = weighted_hinge_loss(ps_rslt, neg_rslt, safety_margin_size, weight)\n",
    "        if(loss > 0):\n",
    "            break;\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_hinge_loss(ps_rslt, neg_rslt, safety_margin_size, weight):\n",
    "    return weight * F.relu(safety_margin_size + ps_rslt - neg_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_0:  loss=9312.409759521484\n",
      "epoch_1:  loss=9527.794250488281\n",
      "epoch_2:  loss=9212.543060302734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-45f1935f7804>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mmini_batch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m':  loss='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\lizhi\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\lizhi\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pre-training of discriminator\n",
    "model = Discriminator(20, 10, 1, key_memory, value_memory)\n",
    "total_loss = []\n",
    "optimizer = Adam(model.parameters(), lr= 0.01)\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0.\n",
    "    for batch_id, (x, y) in enumerate(train_loader):\n",
    "        losses = torch.empty(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            losses[i] = D_pretrain_per_datapoint(x[i],y[i], ngtv_spl_by_unifm_dist)\n",
    "\n",
    "        mini_batch_loss = torch.sum(losses)\n",
    "        epoch_loss += float(mini_batch_loss.data)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        mini_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch_' + str(epoch) + ':  loss=' + str(epoch_loss))\n",
    "    total_loss.append(epoch_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_user_id2vec = {}\n",
    "dict_item_id2vec = {}\n",
    "candidate_ngtv_item = torch.randn(100)\n",
    "prob_ngtv_item = torch.randn(100)\n",
    "\n",
    "for i in candidate_ngtv_item:\n",
    "    dict_item_id2vec[i.item()] = torch.randn(1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_0:  loss=-422.4139099121094  reward=  -91.8388442993164\n",
      "epoch_1:  loss=-456.72161865234375  reward=  -98.9530258178711\n",
      "epoch_2:  loss=-460.8658752441406  reward=  -97.71232604980469\n",
      "epoch_3:  loss=-449.4365234375  reward=  -96.67015075683594\n",
      "epoch_4:  loss=-427.617919921875  reward=  -90.51976013183594\n",
      "epoch_5:  loss=-444.9988708496094  reward=  -95.82398986816406\n",
      "epoch_6:  loss=-379.0965270996094  reward=  -77.06893920898438\n",
      "epoch_7:  loss=-435.7945861816406  reward=  -88.72418212890625\n",
      "epoch_8:  loss=-393.2669677734375  reward=  -80.82926940917969\n",
      "epoch_9:  loss=-440.8939514160156  reward=  -89.49713897705078\n",
      "epoch_10:  loss=-410.2823181152344  reward=  -83.31441497802734\n",
      "epoch_11:  loss=-446.33758544921875  reward=  -89.46939849853516\n",
      "epoch_12:  loss=-450.8667297363281  reward=  -88.67179870605469\n",
      "epoch_13:  loss=-398.5673828125  reward=  -78.1609878540039\n",
      "epoch_14:  loss=-432.98089599609375  reward=  -83.81454467773438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-068385f01e2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_ngtv_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mprob_ngtv_item\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_item_id2vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate_ngtv_item\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mprob_ngtv_item\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_ngtv_item\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngtv_item_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG_pretrain_per_datapoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngtv_spl_by_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\lizhi\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-c97d02fe37ee>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mcat_xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mhidden1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_xy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mhidden2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\lizhi\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pre-training of generator\n",
    "\n",
    "gener = generator(20)\n",
    "total_reward = []\n",
    "total_loss = []\n",
    "optimizer_G = Adam(gener.parameters(), lr= 0.01)\n",
    "for epoch in range(50):\n",
    "    epoch_reward = 0.\n",
    "    epoch_loss = 0.\n",
    "    for batch_id, (x, y) in enumerate(train_loader):\n",
    "        reward = torch.empty(x.shape[0])\n",
    "        mini_batch_loss = torch.empty(x.shape[0])\n",
    "        \n",
    "        for data_point in range(x.shape[0]):\n",
    "            #state = x[i]\n",
    "            #action_pool = []\n",
    "            #reward_pool = []\n",
    "            candidate_ngtv_item = torch.randn(100)\n",
    "            prob_ngtv_item = torch.randn(100)\n",
    "            for i in candidate_ngtv_item:\n",
    "                dict_item_id2vec[i.item()] = torch.randn(1,20)\n",
    "            \n",
    "            for i in range(candidate_ngtv_item.shape[0]):\n",
    "                prob_ngtv_item[i] = gener(x[data_point].unsqueeze(0), dict_item_id2vec[candidate_ngtv_item[i].item()])\n",
    "            prob_ngtv_item = torch.softmax(prob_ngtv_item,0)\n",
    "            reward[data_point], ngtv_item_id = G_pretrain_per_datapoint(x[data_point], ngtv_spl_by_generator, model)\n",
    "            mini_batch_loss[data_point] = (-reward[data_point]) * torch.log(prob_ngtv_item[ngtv_item_id])\n",
    "        rewards = torch.sum(reward)\n",
    "        mini_batch_losses = torch.sum(mini_batch_loss)\n",
    "        epoch_loss = epoch + float(mini_batch_losses.data)\n",
    "        epoch_reward = epoch + float(rewards.data)\n",
    "        \n",
    "        gener.zero_grad()\n",
    "        mini_batch_losses.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    print('epoch_' + str(epoch) + ':  loss=' + str(epoch_loss) + '  reward=  ' + str(epoch_reward))\n",
    "    total_loss.append(epoch_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
