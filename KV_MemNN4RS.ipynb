{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H = 64, 20, 10\n",
    "safety_margin_size = 3\n",
    "#total_item_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return (a-b).pow(2).sum(1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(user):\n",
    "    item = torch.randn(1, D_in)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_memory = torch.randn(H, D_in, requires_grad=True)\n",
    "key_memory = torch.randn(H, D_in, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(500, D_in)\n",
    "y = torch.randn(500, D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, N, key_memory, value_memory):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.key_memory = Parameter(torch.Tensor(H, D_in))\n",
    "        self.value_memory = Parameter(torch.Tensor(H, D_in))\n",
    "        self.key_memory.data = key_memory\n",
    "        self.value_memory.data = value_memory\n",
    "        #self.value_memory.requires_grad=True\n",
    "        self.fc_erase = nn.Linear(D_in, D_in)\n",
    "        self.fc_update = nn.Linear(D_in, D_in)\n",
    "        self.D_in = D_in\n",
    "        self.N = N\n",
    "        self.H = H\n",
    "    def forward(self, user, item):\n",
    "        output_list = []\n",
    "        e_I = torch.ones(self.H, self.D_in)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            #Memory Adderssing\n",
    "            Attention_weight = torch.empty(self.H)\n",
    "\n",
    "            for j in range(self.H):\n",
    "                Attention_weight[j] = -euclidean_distance(user[i].unsqueeze(0), self.key_memory[j,:])\n",
    "\n",
    "\n",
    "            #select value memory by attention\n",
    "\n",
    "            Attention_weight = Attention_weight.softmax(0)\n",
    "            s = Attention_weight.matmul(self.value_memory)\n",
    "\n",
    "            output = euclidean_distance(s, item[i].unsqueeze(0))\n",
    "            output_list.append(output)\n",
    "            \n",
    "            #update value memory by item vector\n",
    "            e_t = self.fc_erase(item[i].unsqueeze(0)).sigmoid()\n",
    "            self.value_memory.data = self.value_memory * (e_I - Attention_weight.unsqueeze(0).t().matmul(e_t))\n",
    "\n",
    "            a_t = self.fc_update(item[i].unsqueeze(0)).tanh()\n",
    "            self.value_memory.data = self.value_memory + Attention_weight.unsqueeze(0).t().matmul(a_t)\n",
    "        \n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 24)\n",
    "        self.fc2 = nn.Linear(24, 36)\n",
    "        self.fc3 = nn.Linear(36, 1) \n",
    "    def forward(self, user, item):\n",
    "        output_list = []\n",
    "        e_I = torch.ones(self.H, self.D_in)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            #Memory Adderssing\n",
    "            Attention_weight = torch.empty(self.H)\n",
    "\n",
    "            for j in range(self.H):\n",
    "                Attention_weight[j] = -euclidean_distance(user[i].unsqueeze(0), self.key_memory[j,:])\n",
    "\n",
    "\n",
    "            #select value memory by attention\n",
    "\n",
    "            Attention_weight = Attention_weight.softmax(0)\n",
    "            s = Attention_weight.matmul(self.value_memory)\n",
    "\n",
    "            output = euclidean_distance(s, item[i].unsqueeze(0))\n",
    "            output_list.append(output)\n",
    "            \n",
    "            #update value memory by item vector\n",
    "            e_t = self.fc_erase(item[i].unsqueeze(0)).sigmoid()\n",
    "            self.value_memory.data = self.value_memory * (e_I - Attention_weight.unsqueeze(0).t().matmul(e_t))\n",
    "\n",
    "            a_t = self.fc_update(item[i].unsqueeze(0)).tanh()\n",
    "            self.value_memory.data = self.value_memory + Attention_weight.unsqueeze(0).t().matmul(a_t)\n",
    "        \n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator(20, 10, 1, key_memory, value_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_datapoint(user, item):\n",
    "    ps_rslt = model(user.unsqueeze(0),item.unsqueeze(0))[0]\n",
    "    neg_rslt = model(user.unsqueeze(0), generator(user))[0]\n",
    "    return ps_rslt, neg_rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use normal hinge_loss before implement of generator\n",
    "def weighted_hinge_loss(ps_rslt, neg_rslt, safety_margin_size):\n",
    "    return F.relu(safety_margin_size + ps_rslt - neg_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = []\n",
    "optimizer = Adam(model.parameters(), lr= 0.01)\n",
    "for epoch in range(50):\n",
    "    losses = torch.empty(5)\n",
    "\n",
    "    for i in range(5):\n",
    "        ps_rslt, neg_rslt = train_per_datapoint(x[i],y[i])\n",
    "        losses[i] = weighted_hinge_loss(ps_rslt, neg_rslt, 3)\n",
    "\n",
    "    mini_batch_loss = torch.sum(losses)\n",
    "    total_loss.append(float(mini_batch_loss.data))\n",
    "\n",
    "    model.zero_grad()\n",
    "    mini_batch_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.893839836120605,\n",
       " 12.60797119140625,\n",
       " 13.931467056274414,\n",
       " 13.747971534729004,\n",
       " 10.971602439880371,\n",
       " 15.503976821899414,\n",
       " 11.689532279968262,\n",
       " 14.63906192779541,\n",
       " 12.685401916503906,\n",
       " 13.13146686553955,\n",
       " 14.879408836364746,\n",
       " 15.143271446228027,\n",
       " 11.764472961425781,\n",
       " 14.215774536132812,\n",
       " 15.724404335021973,\n",
       " 13.589129447937012,\n",
       " 14.119624137878418,\n",
       " 13.775588035583496,\n",
       " 12.62105655670166,\n",
       " 14.591506958007812,\n",
       " 12.706850051879883,\n",
       " 11.98979663848877,\n",
       " 14.428289413452148,\n",
       " 15.281702995300293,\n",
       " 14.493289947509766,\n",
       " 18.07111930847168,\n",
       " 13.574992179870605,\n",
       " 15.360875129699707,\n",
       " 14.06322193145752,\n",
       " 13.372254371643066,\n",
       " 13.445112228393555,\n",
       " 14.595959663391113,\n",
       " 16.539819717407227,\n",
       " 12.95510482788086,\n",
       " 14.256826400756836,\n",
       " 14.360271453857422,\n",
       " 13.74833869934082,\n",
       " 14.389134407043457,\n",
       " 11.892964363098145,\n",
       " 12.730216979980469,\n",
       " 13.607656478881836,\n",
       " 13.281892776489258,\n",
       " 12.306159973144531,\n",
       " 13.009481430053711,\n",
       " 13.639678955078125,\n",
       " 13.911109924316406,\n",
       " 13.194938659667969,\n",
       " 12.223461151123047,\n",
       " 16.161209106445312,\n",
       " 14.63510799407959]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
